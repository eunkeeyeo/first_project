## 배치 사이즈에 따른 변화
커지면 한번에 처리하는 양이 많아지니까 시간은 줄고 정확도는 떨어진다.
정확도는 0.1% 정도씩 움직이는데, 처리 시간은 훅훅 줄어드니까. 배치를 크게 가져가는게 좋다?
epoch 10 이상 반복하면, 정확도 1-2퍼센트 성장.


 + optimizer에 대한 메모. ( http://dmqa.korea.ac.kr/activity/seminar/298 , https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam, tensorflow module python optimizer 이렇게 검색)
어떤 데이터를 가지고 기울기를 구할 것인가?
SGD : 하나의 데이터만을 사용 
BGD(batch gradient descent : 전체 데이터를 사용.
MSGD (Mini-batch Gradient Descent) : 일부 데이터를 사용.

+ 추가로 생긴 방법들.
Momentum : 이전 기울기를 반영해 현재를 업데이트. (local minima, saddle point의 차이점?)
Adaptive stepsize : 파라미터 별로 stepsize를 다르게 적용.
Adam : Momentum + Adaptive stepsize .
Lookahead : 일정 횟수마다 멈춘 뒤 다시 시작했던 파라미터가 있는 방향으로 일정비율만큼 되돌아감. 공부해보고 사용해 보도록 하자.


## optimizer에 따른 변화
사용하던 것은 adam 이었고, 꽤나 좋은 모델이더라?
sdg를 사용해 보니 확실히 정확도가 떨어짐. 시간의 차이는 -7%.


## epochs에 따른 변화
20 -> 40 ...? 시간만..




## Dense에 따른 변화
#1 정확도 0.1 감소, 시간 15% 감소.
#3 정확도 0.1 증가, 시간 11% 증가.

